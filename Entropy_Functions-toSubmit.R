## Function for Creating Particles ----------------

#' Create 0-dim particles
#' 
#' This function creates a data set of dimensionless particles,
#' i.e. without geometry. Each particle is only described by its
#' content in a series of components.
#'
#' @param K number of particles desired
#' @param J number of components desired
#' @param l approx proportion of zero components in each column
#'
#' @return a matrix of (K,J)-"masses" each between 0 and 100.
#'
#' @examples
#' # 100 particles, 3 minerals, 20% zeros:
#' particle_create(K=100, J=3, l=0.2)
#' # 10 particles, 2 minerals, no zeros:
#' particle_create(K=10, J=2, l=0)
particle_create <- function(J,K,l){
  # generate a few particles more to ensure n
  fk = 1+(l^J)*4
  n_ext = floor(K*fk) 
  # generate initial compositions
  rnd_comps <- runif(n_ext*K, min = 0, max = 100)
  part <- matrix(data = rnd_comps, ncol = J)
  # "liberate" some of them by making some components 0
  part <- apply(part,2,function(x){
    idx <- sample(c(rep(0,(1-l)*length(x)),rep(1,l*length(x))))
    x[idx == 1] <- 0
    return(x)
  })
  # select the first "n" of them that are not fully zero 
  part <- part[rowSums(part)>0,][1:K,]
  colnames(part) <- LETTERS[1:J]
  return(part)
}





## Single Particle Comminution Functions ----------------


#' Comminute one particle by full liberation
#' 
#' Split the particle in J children, where J is the number of
#' components of the system. Each child particle is then fully
#' liberated. Already liberated particles are left unchanged.  
#'
#' @param part a vector of J-masses of components of that particle 
#' @param ... unused, here just consistency with other comminution functions
#'
#' @return A new matrix of (k, J) masses, with is between 1 and J,
#' where for each row (each particle) only one column is non-zero.
#' These correspond to the sizes of the fully liberated progeny 
#' particles.
#'
#' @examples
#' x = particle_create(J=10, K=2, l=0)
#' x_1 = liberation_breakage(part=x[1,])
#' dim(x_1) # should be c(2,2)
liberation_breakage <- function(part, ...){
  # check if particle is liberated:
  li <- length(which(part == 0))
  ll <- length(part)
  if (ll-li == 1){
    # just return liberated particle
    res <- matrix(data = part, ncol = ll)
  }else{
    # distribute components over new particles
    res <- matrix(data = 0, nrow = ll, ncol = ll)
    diag(res) = part}
  res <- res[rowSums(res)>0,]
  return(res)
}




#' Comminute one particle by random breakage
#' 
#' Split the particle in `nchild` children. Each child particle is 
#' generated by distributing the mass of each component across
#' the number of children, following a Dirichlet distribution with
#' random parameter vector.  
#'
#' @param part a vector of J-masses of components of that particle 
#' @param nchild number of desired children; defaults to `NULL`, which
#' implies randomly generating between 2 and 5 progeny particles
#'
#' @return A new matrix of (nchild, J) masses, where for each row 
#' gives the composition of a child particle in the original 
#' components.
#'
#' @examples
#' x = particle_create(J=2, K=4, l=0)
#' x_1 = random_breakage(x[1,], nchild=6)
#' # does breakage preserve mass?
#' colSums(x_1) - x[1,]
random_breakage <- function(part, nchild=NULL){
  # dimensions:
  K = length(part)
  # how many particles should be generated?
  if(is.null(nchild)){
    n_child <- round(runif(n = 1, min = 1.5, max =  5.5), digits = 0)  
  }else{
    n_child = nchild
  }
  
  # particles will be generated according to a Dirichlet distribution
  ## generate the parameter of the Dirichlet:
  a <- runif(1, min = 0.01, max = 5)
  
  ## calculate splitting weights to distribute each component in progeny
  ww <- compositions::rDirichlet.rcomp(n=K, alpha=rep(a, n_child) )
   
  #compute child particles
  res <- t(unclass(ww)) %*% diag(part)
  return(res)
}

#' Leave one particle unchanged
#' 
#' Leave a particle unmodified during breakage  
#'
#' @param part a vector of J-masses of components of that particle 
#' @param ... unused, here just consistency with other comminution functions
#'
#' @return direcly returns `part`, meaning that the particle did not get
#' comminuted. 
no_breakage = function(part, ...){
  return(part)
}


## Global Comminution Functions ----------------

#' Comminute all particles in the data set.
#'
#' @param feed_part a (K,J)-matrix of masses of components by 
#' particle
#' @param prob_rnd_break proportion of random breakage; defaults to 0.2
#' @param process_fraction proportion of particles being actually
#' comminuted, defaults to 1=100% (i.e. all particles are comminuted)
#'
#' @return A named list of two elements, `parent` and `progeny`.
#' In `parent`, a copy of `feed_part` is provided. In `progeny`,
#' a new matrix of (K', J) masses is returned, where each row gives 
#' the composition of a child particle in the original components. 
#'
#' @examples
#' x = particle_create(J=10, K=3, l=0)
#' x_1 = comminute(feed_part=x)
#' # do we preserve mass?
#' colSums(x_1$parent) - colSums(x_1$progeny)
#' head(x_1$parent)
#' head(x_1$progeny)
comminute <- function(part, prob_rnd_break=0.2, process_fraction=1){
  # dimensions
  K = nrow(part)
  # which function will be applied to each particle?
  # if not all particles are to be processed
  if(process_fraction<1){
    # select which particles will be processed
    part_size = rowSums(part) # proportional to its size
    p = part_size/sum(part_size)
    tk = sample(1:K, size = floor(process_fraction*K), prob = p )
    id = rep(3, K)
    # chosee the breakage style of those breaking
    id[tk] = sample(c(1:2), size = length(tk), replace = T, 
                          prob = c(1-prob_rnd_break, prob_rnd_break))
    breakage_fun = c(liberation_breakage, random_breakage, no_breakage)[id]
  }else{ # or else break all particles
    breakage_fun = sample(c(liberation_breakage, random_breakage), 
                          size = K, replace = T, 
                          prob = c(1-prob_rnd_break, prob_rnd_break))
  }
  
  # calculate child particles per parent particle
  res <- purrr::map2(breakage_fun, 1:K, ~.x(part=part[.y,]))
  
  # collect all progeny together
  res_total <- do.call(rbind, res)
  
  #delete 0 Particles and set colnames
  res_total <-  res_total[rowSums(res_total) > 0,] 
  colnames(res_total) <- colnames(part)
  
  # return input and output
  res <- list(
    parent = part,
    progeny = res_total
  )
  return(res)
}



## Global Separation Function ----------------

#' Apply a separation process to all particles of the data set
#'
#' @param feed feed particles, matrix of (K,J) masses per 
#' particle and component
#' @param phase.property property of each phase (density in kg/dm3) 
#' @param cut.point cut point of the separation function
#' @param alpha sharpness of the Rogers' separation model, defaults to 5.
#' Special cases for `alpha=NA` (perfect splitting) and `alpha=Inf` 
#' (perfect separation, with sign!)
#'
#' @return In order to be able to enable different calculations,
#' the function provides a series of different results, in a named 
#' list:
#' 
#' * `feed` returns the input `feed` particle set
#' * `products` returns a named list with the particles sent to 
#' `concentrate` and to `tailings`, in each case a matrix of J columns
#' and as many rows as particles went to that stream 
#' * `prop.vec` returns a K-vector with the computed effective particle
#' property, i.e. particle density
#' * `prob.vec` returns the partition coefficient computed for each 
#' particle
#' * `idx` returns an index (1 or 2) indicating to which stream each
#' feed particle is sent, i.e. `products$concentrate` is equal to 
#' `feed[idx==1,]`
#'
#' @examples
#' x = particle_create(J=10, K=3, l=0)
#' y = separate(x, phase.property=c(A=2.7, B=3.5, C=7), cut.point=6, alpha=NA)
#' y$products$concentrate
#' y$feed[y$idx==1,]
#' y
#' separate(x, phase.property=c(A=2.7, B=3.5, C=7), cut.point=6, alpha=1)$idx
#' separate(x, phase.property=c(A=2.7, B=3.5, C=7), cut.point=6)$idx
separate <- function(feed, phase.property, cut.point, alpha = 5){
  # calculate property vector volume weighted - add option for mass weighted
  feed_volumes <-  sweep(feed,2,phase.property,"/")
  feed.property <- rowSums(feed)/rowSums(feed_volumes) # %*% phase.property
  
  sep.type = ifelse(is.na(alpha), 
                    "ideal.split", 
                    ifelse(is.finite(alpha), 
                           "rogers", 
                           "ideal.sep")
              )
  
  if (sep.type == "ideal.sep") {
    # ideal separation == vertical jump from 0 to 1 at cut point
    prob.vec <- rep(0,nrow(feed))
    prob.vec[feed.property*sign(alpha) > cut.point*sign(alpha)] <- 1
    res <- list(
      feed = feed,
      products = list(
        concentrate = feed[feed.property > cut.point,],
        tailings = feed[feed.property < cut.point,]
      ),
      prop.vec = feed.property,
      prob.vec = prob.vec, 
      idx = 1 + (feed.property <= cut.point)
    )
  } else if (sep.type == "ideal.split") {
    # ideal splitting == every particle has the probability of 0.5
    prob.vec = rep(0.5, nrow(feed))
    
    res <- list(
      feed = feed,
      products = list(
        concentrate = feed[prob.vec > 0.5,],
        tailings = feed[prob.vec <= 0.5,]
      ),
      prop.vec = feed.property,
      prob.vec = prob.vec,
      idx = 1 + (prob.vec <= 0.5)
    )
  } else if (sep.type == "rogers") {
    # rogers separation function with alpha as uncertainty
    
    #defining rogers function
    rogers.func <- function(x,a,alpha,xt){
      (1-a)/(1+xt/x*exp(alpha*(1-(x/xt)^3)))+a}
    #compute 
    tvals <- rogers.func(x = feed.property, a = 0, alpha = alpha, xt =    cut.point)
    
    probs <- runif(nrow(feed),min = -0.5,max = 0.5)
    probs 
    #rbinom
    t <- probs+tvals
    
    res <- list(
      feed = feed,
      products = list(concentrate = feed[t > 0.5,],
                      tailings = feed[t <= 0.5,]),
      prop.vec = feed.property,
      prob.vec = tvals,
      idx = 1+(t <= 0.5)
    )
  }
  return(res)
}



## Entropy Calculation Function ----------------

#' Internal function to compute logs if not zero
#'
#' @param x argument, can be a vector 
#'
#' @return component-wise, log(x[i]) if x[i]!=0, or 0 otherwise
.log <-  function(x) ifelse(x>0,log(x),0)

#' Internal function to compute entropy of a vector
#'
#' @param x argument, needs to be a vector 
#'
#' @return -sum(x*log(x)), i.e. Shannon entropy. No effort is
#' done to ensure that sum(x)==1, that is up to the user.
.entropy <-  function(x) -x %*% .log(x)

#' Computes entropy contributions for one single batch
#'
#' Computes the two decompositions in two contributions 
#' of the extensive entropy of a single batch of particles.
#'
#' @param part a matrix of particles, i.e. a matrix with 
#' `(K,J)`-masses, `K` being the number of particles and `J`
#' the number of components
#'
#' @return A data set (class "tibble") with the following 
#' columns:
#' 
#' * `.id` character, identificator of the decomposition
#' * `contribution` character, identificator of entropy 
#' contribution, e.g. one of `c0j0` (bulk compositional entropy),
#' `p00k` (bulk particle size distribution entropy),
#' `c0jk` (particle compositional entropy) and
#' `p0jk` (component mass particle distribution entropy)
#' * `entropy` value of the calculated extensive entropy
#'
#' @examples
#' x = particle_create(J=10, K=3, l=0)
#' compute_batch_entropy(x)
compute_batch_entropy = function(part){
 ## compute p00k*c0jk: ##
  
  # compute the absolute size of each particle
  abs.size <-  rowSums(part)
  # compute size distribution for each particle
  p00k <-  abs.size/sum(abs.size)
  # compute particle size distribution entropy and summing it up
  p00k_ent <-  .entropy(p00k)
  # compute compositions of particles 
  c0jk <-  as.matrix(sweep(part, 1, abs.size, "/"))
  # compute composition entropy for each particle (= intergrowth)
  c0jk_ent <-  apply(c0jk, 1, .entropy)
  # weight compositional entropy by size distribution
  c0jk_ent <-  c0jk_ent %*% p00k
  
  res_p00k.c0jk <- c(p00k = p00k_ent,c0jk = c0jk_ent)
  
 ## compute c0j0*p0jk: ##
  
  # compute absolute amount of each component
  abs.size <-  colSums(part)
  # compute bulk composition
  c0j0 <-  abs.size/sum(abs.size)
  # compute compositional entropy 
  c0j0_ent <- .entropy(c0j0)
  # compute distribution of component over particles
  p0jk <-  as.matrix(sweep(part, 2, abs.size, "/"))
  # compute component distribution entropy
  p0jk_ent <-  apply(p0jk, 2, .entropy)
  # weight component distribution entropy by component
  p0jk_ent <- c0j0 %*% p0jk_ent 
  
 ## prepare and return results: ##
  res_c0j0.p0jk <- c(c0j0 = c0j0_ent, p0jk = p0jk_ent)
  
  res <- list(
    c0j0.p0jk = res_c0j0.p0jk,
    p00k.c0jk = res_p00k.c0jk
  )
  
  res <- dplyr::bind_rows(res, .id = ".id") %>% 
    tidyr::gather(key = contribution, value = "value",-.id) %>% 
    na.omit
  
  return(res)
}



#' Computes entropy contributions for all output batches at a stage
#'
#' Computes the six decompositions in three contributions 
#' of the extensive entropy of all output batches of particles 
#' at a stage.
#'
#' @param part a set of particles in several output streams, as 
#' returned by a call to `separate(x,...)` with `x` a feed particle
#' data set.
#'
#' @return A data set (class "tibble") with the following columns
#' * `.id` string describing the entropy decomposition order
#' * `contribution` string describing the entropy source
#' * `value` entropy contribution value.
#' As an extra attribute (named `mass_pulls`), the output contains as
#' well the mass pulls of each component onto each output stream, to
#' enable further calculations of grade and recovery.
#'
#' @examples
#' x = particle_create(J=10, K=2, l=0)
#' y = separate(x, phase.property=c(A=2.7, B=7), cut.point=4, alpha=NULL)
#' compute_stage_entropy(y)
compute_stage_entropy <- function(part){
 ## preparations ##
  # required package
  requireNamespace("purrr", quietly = TRUE)
  # particle-wise partition coefficient to concentrate and tailings
  ti0k <- matrix(nrow = nrow(part$feed),ncol = 2)
  ti0k[,1] <- part$prob.vec
  ti0k[,2] <- 1-part$prob.vec
  prob.l <- list(concentrate=ti0k[,1], tailings=ti0k[,2])
  
 ## calculate aggregated and intensive quantities: ##
  # mass partition coefficients
  ti00 <- colSums(ti0k*rowSums(part$feed))
  N000 <- sum(ti00)
  ti00 <- ti00/N000
  # bulk composition per batch
  Nijk <- map(prob.l, ~ .x*part$feed )
  Nij0 <- map(Nijk, colSums )
  cij0 <- map(Nij0, ~ .x/sum(.x))
  # distribution of each component over particles:
  pijk <- map2(Nijk, Nij0, ~ sweep(.x,2,.y,"/"))  
  
  # stage feed particle size distribution:
  N00k <- rowSums(part$feed)
  p00k <- N00k/N000  
  # particle size distribution per batch:
  Ni0k <- map(Nijk, rowSums )
  pi0k <- map(Ni0k, ~ .x/sum(.x))   
  # composition of each particle class:
  cijk <- map2(Nijk, Ni0k, ~ sweep(.x, 1, .y, "/"))
  
  # stage feed composition
  N0j0 <- colSums(part$feed)
  c0j0 <- N0j0/N000
  # component-wise mass dispersion over stage feed particles 
  p0jk <- sweep(part$feed, 2, N0j0, "/")
  # component- & particle-wise partition coefficients 
  tijk <- ti0k # might be different in other settings!
  
  # particle composition at the stage feed
  c0jk <- sweep(part$feed, 1, N00k, "/")

  # bulk component-wise partition
  Nij0 <- map_dfr(Nijk, colSums) 
  tij0 <- sweep(Nij0, 2, N0j0, "/")
  
  
 ## calculating aggregation ti00*cij0*pijk: ##
  # entropy of mass dispersion into products
  ti00_ent <- .entropy(ti00)
  # compositional dispersion by product
  cij0_ent <- map_dbl(cij0, .entropy)
  cij0_ent <- cij0_ent %*% ti00
  # component mass dispersion over particles per product
  pijk_ent <- map(pijk, ~ apply(.x, 2, .entropy))
  pijk_ent <- map2_dbl(cij0, pijk_ent, ~ .x %*% .y)
  pijk_ent <- pijk_ent %*% ti00
  
  res_sep_ti00.cij0.pijk = c(ti00 = ti00_ent, cij0 = cij0_ent, pijk = pijk_ent) 
  
 ## calculating aggregation p00k*ti0k*cijk: ##
  # entropy per particle size distribution
  p00k_ent <- .entropy(p00k)
  # entropy of particle-wise partitions: 
  ti0k_ent <- p00k %*% apply(ti0k, 1, .entropy)
  # entropy of particle composition
  cijk_ent <- map_dfc(cijk, ~ apply(.x, 1, .entropy)  ) 
  cijk_ent <- p00k %*% rowSums( cijk_ent * ti0k  )
  
  res_sep_p00k.ti0k.cijk <- c(p00k = p00k_ent,ti0k = ti0k_ent, cijk = cijk_ent)
  
 ## calculating aggregation c0j0*p0jk*tijk: ##
  # stage compositional bulk entropy
  c0j0_ent <- .entropy(c0j0)
  # stage entropy of component distribution over particles
  p0jk_ent <- apply(p0jk, 2, .entropy)
  p0jk_ent <- c0j0 %*% p0jk_ent
  # component- & particle-wise partition coefficient entropy
  tijk_ent <- apply(tijk, 1, .entropy)
  tijk_ent <- apply(p0jk, 2, function(x) x %*% tijk_ent)
  tijk_ent <- c0j0 %*% tijk_ent
  
  res_sep_c0j0.p0jk.tijk <- c(c0j0 = c0j0_ent, p0jk = p0jk_ent, tijk = tijk_ent)
  
 ## calculating aggregation p00k.c0jk.tijk: ##
  # stage feed particle compositional entropy
  c0jk_ent <- apply(c0jk, 1, .entropy)
  c0jk_ent <- p00k %*% c0jk_ent 
  
  res_sep_p00k.c0jk.tijk <- c(p00k = p00k_ent, c0jk = c0jk_ent, tijk = tijk_ent)
  
 ## calculating aggregation ti00.pi0k.cijk: ##
  # entropy of particle size dispersion per batch
  pi0k_ent <- sapply(pi0k, .entropy)
  pi0k_ent <- pi0k_ent %*% ti00 
  
  res_sep_ti00.pi0k.cijk <- c(ti00 = ti00_ent, pi0k = pi0k_ent, cijk = cijk_ent)
  
 ## calculating aggregation c0j0.tij0.pijk: ###
  # bulk component partition coeffient entropy
  tij0_ent <- apply(tij0, 2, .entropy)
  tij0_ent <- c0j0 %*% tij0_ent
  
  res_sep_c0j0.tij0.pijk <- c(c0j0 = c0j0_ent,tij0 = tij0_ent,pijk = pijk_ent)
  
 ## return results: ##  
  
  res <- list(
    ti00.cij0.pijk = (res_sep_ti00.cij0.pijk),
    p00k.ti0k.cijk = (res_sep_p00k.ti0k.cijk),
    p00k.c0jk.tijk = (res_sep_p00k.c0jk.tijk),
    c0j0.p0jk.tijk = (res_sep_c0j0.p0jk.tijk),
    ti00.pi0k.cijk = (res_sep_ti00.pi0k.cijk),
    c0j0.tij0.pijk = (res_sep_c0j0.tij0.pijk)
  )  
  
  
  rs <- dplyr::bind_rows(res, .id = ".id") %>% 
    tidyr::gather(key = contribution, value = "value",-.id) %>% 
    na.omit()
  
  attr(rs, "mass_pulls") = Nij0
  
  return(rs)
}

